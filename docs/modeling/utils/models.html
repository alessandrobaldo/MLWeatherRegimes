<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>modeling.utils.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>modeling.utils.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pandas as pd
from scipy.spatial import distance
from modeling.utils.tools import *
from functools import wraps, partial
from multiprocessing.dummy import Pool
import pickle
from copy import deepcopy
import os

from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.base import BaseEstimator, ClusterMixin
from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.utils import check_random_state



def load_estimator(path_to_file):
    &#39;&#39;&#39;
    Method to load an estimator from the pickle file
    Args:
        path_to_file: file path to the pickle file

    Returns:
        the loaded estimator
    &#39;&#39;&#39;
    with open(path_to_file, &#39;rb&#39;) as pickle_file:
        estimator = pickle.load(pickle_file)

    return estimator


class KernelKMeans(BaseEstimator, ClusterMixin):
    &#34;&#34;&#34;
    Kernel K-means

    Reference
    ---------
    Kernel k-means, Spectral Clustering and Normalized Cuts.
    Inderjit S. Dhillon, Yuqiang Guan, Brian Kulis.
    KDD 2004.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=3, max_iter=50, tol=1e-3, random_state=None,
                 kernel=&#34;linear&#34;, gamma=None, degree=3, coef0=1,
                 kernel_params=None, verbose=0):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.kernel = kernel
        self.gamma = gamma
        self.degree = degree
        self.coef0 = coef0
        self.kernel_params = kernel_params
        self.verbose = verbose

    @property
    def _pairwise(self):
        return self.kernel == &#34;precomputed&#34;

    def _get_kernel(self, X, Y=None):
        if callable(self.kernel):
            params = self.kernel_params or {}
        else:
            params = {&#34;gamma&#34;: self.gamma,
                      &#34;degree&#34;: self.degree,
                      &#34;coef0&#34;: self.coef0}
        return pairwise_kernels(X, Y, metric=self.kernel,
                                filter_params=True, **params)

    def fit(self, X, y=None, sample_weight=None):
        n_samples = X.shape[0]

        K = self._get_kernel(X)

        sw = sample_weight if sample_weight else np.ones(n_samples)
        self.sample_weight_ = sw

        rs = check_random_state(self.random_state)
        self.labels_ = rs.randint(self.n_clusters, size=n_samples)

        dist = np.zeros((n_samples, self.n_clusters))
        self.within_distances_ = np.zeros(self.n_clusters)

        for it in range(self.max_iter):
            dist.fill(0)
            self._compute_dist(K, dist, self.within_distances_,
                               update_within=True)
            labels_old = self.labels_
            self.labels_ = dist.argmin(axis=1)

            # Compute the number of samples whose cluster did not change
            # since last iteration.
            n_same = np.sum((self.labels_ - labels_old) == 0)
            if 1 - float(n_same) / n_samples &lt; self.tol:
                if self.verbose:
                    print(&#34;Converged at iteration&#34;, it + 1)
                break

        self.X_fit_ = X

        return self

    def _compute_dist(self, K, dist, within_distances, update_within):
        &#34;&#34;&#34;Compute a n_samples x n_clusters distance matrix using the
        kernel trick.&#34;&#34;&#34;
        sw = self.sample_weight_

        for j in range(self.n_clusters):
            mask = self.labels_ == j

            if np.sum(mask) == 0:
                raise ValueError(&#34;Empty cluster found, try smaller n_cluster.&#34;)

            denom = sw[mask].sum()
            denomsq = denom * denom

            if update_within:
                KK = K[mask][:, mask]  # K[mask, mask] does not work.
                dist_j = np.sum(np.outer(sw[mask], sw[mask]) * KK / denomsq)
                within_distances[j] = dist_j
                dist[:, j] += dist_j
            else:
                dist[:, j] += within_distances[j]

            dist[:, j] -= 2 * np.sum(sw[mask] * K[:, mask], axis=1) / denom

    def predict(self, X):
        K = self._get_kernel(X, self.X_fit_)
        n_samples = X.shape[0]
        dist = np.zeros((n_samples, self.n_clusters))
        self._compute_dist(K, dist, self.within_distances_,
                           update_within=False)
        return dist.argmin(axis=1)


def extract_regimes(anomaly, method=&#39;kmeans&#39;, nb_regimes=5, **kwargs):
    &#39;&#39;&#39;
    Method clustering anomalies in different weather regimes
    Args:
        anomaly: a pandas DataFrame containing an historical series of anomalies
        clustering_algo: clustering algorithm to adopt. The class should expose the methods .fit(), .predict(), .fit_predict(), .fit_transform()
        nb_regimes: number of different weather regimes to be identified, if &#39;estimator&#39; in **kwargs, this parameter is ignored
        **kwargs: a dictionary of further parameters, like the a pre_trained estimator

    Returns:
        an array of regimes associated to each grid in the time series
    &#39;&#39;&#39;
    if method == &#39;kmeans&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = KMeans(n_clusters=nb_regimes, random_state=42,
                                     tol=1e-5, n_init=50)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.inertia_, clustering_algo

    elif method == &#34;kernel_kmeans&#34;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = KernelKMeans(n_clusters=nb_regimes, random_state=42,
                                           tol=1e-5, n_init=50)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.inertia_, clustering_algo

    elif method == &#39;bayesian_gmm&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = BayesianGaussianMixture(n_components=nb_regimes, random_state=42, n_init=10,
                                                      covariance_type=&#39;full&#39; if &#39;covariance_type&#39; not in kwargs else
                                                      kwargs[&#39;covariance_type&#39;],
                                                      reg_covar=1e-3, max_iter=1000)
            clustering_algo.fit(anomaly)

        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        probas = clustering_algo.predict_proba(anomaly)

        return probas, clustering_algo.lower_bound_, clustering_algo.means_, clustering_algo.covariances_, clustering_algo

    elif method == &#39;gmm&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = GaussianMixture(n_components=nb_regimes, random_state=42, n_init=10,
                                              covariance_type=&#39;full&#39; if &#39;covariance_type&#39; not in kwargs else kwargs[
                                                  &#39;covariance_type&#39;],
                                              reg_covar=1e-3, max_iter=1000)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        probas = clustering_algo.predict_proba(anomaly)

        return probas, clustering_algo.lower_bound_, clustering_algo.means_, clustering_algo.covariances_, clustering_algo

    elif method == &#39;spectral&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = SpectralClustering(n_clusters=nb_regimes, random_state=42, n_init=50,
                                                 affinity=&#39;rbf&#39; if &#39;affinity&#39; not in kwargs else kwargs[&#39;affinity&#39;])
            clustering_algo.fit(anomaly)

        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.affinity_matrix_, clustering_algo

    else:
        pass


def bic_score(X, labels, centroids):
    &#39;&#39;&#39;
    Method to compute the BIC (Bayesian Information Criterion)
    Args:
        X: the dataset on which evaluating the BIC
        labels: labels associated to the samples in X
        centroids: the centroids associated to X

    Returns:
        the BIC score associated to the estimator
    &#39;&#39;&#39;
    eps = 1e-7
    m = centroids.shape[0]
    n = np.zeros((m,))
    hist = np.bincount(labels)
    n[:len(hist)] = hist

    N, D = X.shape

    const_term = 0.5 * m * np.log(N) * (D + 1)
    cl_var = (1. / (N - m) / D) * sum([
        sum(distance.cdist(X[np.where(labels == i)], [centroids[i]], &#39;euclidean&#39;) ** 2) for i in range(m)
    ])

    return np.sum([n[i] * np.log(n[i] + eps) -
                   n[i] * np.log(N) -
                   ((n[i] * D) / 2) * np.log(2 * np.pi * cl_var + eps) -
                   ((n[i] - 1) * D / 2) for i in range(m)]) - const_term


def get_inertia(X, labels, centroids):
    &#39;&#39;&#39;
    Method to compute the Sum of Squared Errors (SSE)
    Args:
        X: the dataset of clustered points
        labels: the clustering assignments
        centroids: the centroids associated to X
    &#39;&#39;&#39;
    m = centroids.shape[0]
    return np.sum(
        [np.sum(distance.cdist(X[np.where(labels == i)], [centroids[i]], &#39;euclidean&#39;) ** 2) for i in range(m)])


def performance_matrix(estimator, train_X, val_X):
    &#39;&#39;&#39;
    Method to evaluate an estimator, on different evaluation metrics
    Args:
        estimator: the clustering algorithm to be tested, or the path to the pickle file to load the estimator

    Returns:
        a pandas DataFrame comparing several estimators
    &#39;&#39;&#39;

    if isinstance(estimator, str):
        estimator = load_estimator(estimator)

    labels = estimator.fit(train_X).predict(val_X)
    return {
        # &#34;Internal score&#34;: estimator.score(val_X, labels),
        &#34;Number of Clusters&#34;: estimator.get_params()[&#39;n_clusters&#39;
        if &#39;n_clusters&#39; in estimator.get_params() else &#39;n_components&#39;],
        &#34;BIC&#34;: bic_score(val_X, labels,
                         estimator.cluster_centers_ if hasattr(estimator, &#34;cluster_centers_&#34;) else estimator.means_),
        &#34;Silhouette Score&#34;: silhouette_score(val_X, labels),
        &#34;Calinski Harabsz Index&#34;: calinski_harabasz_score(val_X, labels),
        &#34;Inertia&#34;: get_inertia(val_X, labels,
                               estimator.cluster_centers_ if hasattr(estimator,
                                                                     &#34;cluster_centers_&#34;) else estimator.means_),

    }


@timing
def cross_val(X, method=&#34;kmeans&#34;, scoring=&#34;score&#34;, season = &#34;WINTER&#34;, folder = &#39;&#39;, verbose=True):
    &#39;&#39;&#39;
    Method to perform cross-validation of clustering methods
    Args
        X: pandas DataFrame containing data
        method: clustering method to be validated
        scoring
        season:
        verbose
    &#39;&#39;&#39;

    if method == &#39;kmeans&#39;:
        estimator = KMeans(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;init&#34;: [&#34;k-means++&#34;, &#34;random&#34;],
                  &#34;n_init&#34;: [10, 50], &#34;max_iter&#34;: [100, 300, 1000], &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#34;kernel_kmeans&#34;:
        estimator = KernelKMeans(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;init&#34;: [&#34;k-means++&#34;, &#34;random&#34;],
                  &#34;n_init&#34;: [10, 50], &#34;max_iter&#34;: [100, 300, 1000], &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#39;bayesian_gmm&#39;:
        estimator = BayesianGaussianMixture(random_state=42)
        params = {&#34;n_components&#34;: [4, 5, 6, 7], &#34;covariance_type&#34;: [&#34;full&#34;],
                  &#34;n_init&#34;: [5, 10], &#34;max_iter&#34;: [100, 3000], &#34;init_params&#34;: [&#34;kmeans&#34;, &#34;random&#34;],
                  &#34;tol&#34;: [1e-3, 1e-7],
                  &#34;weight_concentration_prior_type&#34;: [&#34;dirichlet_process&#34;, &#34;dirichlet_distribution&#34;]}

    elif method == &#39;gmm&#39;:
        estimator = GaussianMixture(random_state=42, warm_start=True)
        params = {&#34;n_components&#34;: [4, 5, 6, 7], &#34;covariance_type&#34;: [&#34;full&#34;],
                  &#34;n_init&#34;: [5, 10], &#34;max_iter&#34;: [100, 300, 1000], &#34;init_params&#34;: [&#34;kmeans&#34;, &#34;random&#34;],
                  &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#39;spectral&#39;:
        estimator = SpectralClustering(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;affinity&#34;: [&#34;nearest_neighbors&#34;, &#34;rbf&#34;, &#34;precomputed&#34;],
                  &#34;n_neighbors&#34;: [10, 50], &#34;n_init&#34;: [10, 50]}

    else:
        estimator = None

    def make_generator(parameters):
        &#39;&#39;&#39;
        Method creating a generator on the fly returning all the combinations given the passed parameters
        Args:
            parameters: a dictionary containing the parameters to be passed
        &#39;&#39;&#39;
        if not parameters:
            yield dict()
        else:
            key_to_iterate = list(parameters.keys())[0]
            next_round_parameters = {p: parameters[p]
                                     for p in parameters if p != key_to_iterate}
            for val in parameters[key_to_iterate]:
                for pars in make_generator(next_round_parameters):
                    temp_res = pars
                    temp_res[key_to_iterate] = val
                    yield temp_res

    kf = KFold(n_splits=5, random_state=42, shuffle=True)
    _best_estimator, _best_score, _best_params = None, -np.inf, None

    def get_score(indexes, estimator, X, scoring):
        train_X, val_X = X[indexes[0]], X[indexes[1]]
        try:
            estimator.fit(train_X)
            labels = _est.predict(val_X)

            if scoring == &#34;score&#34;:
                scoring_fn = getattr(estimator, &#34;score&#34;, None)
                if callable(scoring_fn):
                    score = estimator.score(val_X)
            elif scoring == &#34;silhouette&#34;:
                score = silhouette_score(val_X, labels)

            elif scoring == &#34;ch&#34;:
                score = calinski_harabasz_score(val_X, labels)

            elif scoring == &#34;bic&#34;:
                scoring_fn = getattr(estimator, &#34;bic&#34;, None)
                if callable(scoring_fn):
                    score = - estimator.bic(val_X)
                else:
                    score = bic_score(val_X, labels,
                                      estimator.cluster_centers_ if hasattr(estimator,
                                                                            &#34;cluster_centers_&#34;) else estimator.means_)
        except:
            score = np.nan
        return score

    for i, param_grid in enumerate(make_generator(params)):
        _est = estimator.set_params(**param_grid)
        pool = Pool()
        scores = pool.map(partial(get_score, estimator=_est, X=X, scoring=scoring), kf.split(X))

        if verbose:
            print(&#34;\tCombination %d score: %.3f&#34; % (i + 1, np.mean(scores)))

        if np.mean(scores) &gt; _best_score:
            _best_estimator, _best_score, _best_params = deepcopy(_est), np.mean(scores), _est.get_params()

    try:
        os.makedirs(os.path.join(&#34;../models&#34;,season,folder))
    except OSError as oserr:
        pass
    finally:
        _best_estimator.fit(X)
        with open(&#34;../models/&#34; + season + &#34;/&#34; + folder + &#39;/&#39; + method + &#39;_model_&#39; + scoring + &#39;.pkl&#39;, &#39;wb&#39;) as f:
            pickle.dump(_best_estimator, f)

    print(&#34;Validation process ended with score {}\nBest parameters: {}&#34;.format(_best_score, _best_params))
    return _best_estimator

@timing
def get_statistics(folder, train, test):
    &#39;&#39;&#39;
    Method to collect the statistics about the models&#39; performances
    Args:
        folder: name of the folder where there are pickle files of the model
        train: the train dataset used to fit the model
        test: the test dataset used to evaluate the performance on

    Returns:
        A pandas DataFrame containing the statistics

    &#39;&#39;&#39;

    return pd.DataFrame.from_dict({model: performance_matrix(folder + &#39;/&#39; + model,
                                                      train.values, test.values) \
                            for model in os.listdir(folder) if
                            model.endswith(&#39;pkl&#39;)},
                           orient=&#39;index&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="modeling.utils.models.bic_score"><code class="name flex">
<span>def <span class="ident">bic_score</span></span>(<span>X, labels, centroids)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to compute the BIC (Bayesian Information Criterion)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong></dt>
<dd>the dataset on which evaluating the BIC</dd>
<dt><strong><code>labels</code></strong></dt>
<dd>labels associated to the samples in X</dd>
<dt><strong><code>centroids</code></strong></dt>
<dd>the centroids associated to X</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the BIC score associated to the estimator</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bic_score(X, labels, centroids):
    &#39;&#39;&#39;
    Method to compute the BIC (Bayesian Information Criterion)
    Args:
        X: the dataset on which evaluating the BIC
        labels: labels associated to the samples in X
        centroids: the centroids associated to X

    Returns:
        the BIC score associated to the estimator
    &#39;&#39;&#39;
    eps = 1e-7
    m = centroids.shape[0]
    n = np.zeros((m,))
    hist = np.bincount(labels)
    n[:len(hist)] = hist

    N, D = X.shape

    const_term = 0.5 * m * np.log(N) * (D + 1)
    cl_var = (1. / (N - m) / D) * sum([
        sum(distance.cdist(X[np.where(labels == i)], [centroids[i]], &#39;euclidean&#39;) ** 2) for i in range(m)
    ])

    return np.sum([n[i] * np.log(n[i] + eps) -
                   n[i] * np.log(N) -
                   ((n[i] * D) / 2) * np.log(2 * np.pi * cl_var + eps) -
                   ((n[i] - 1) * D / 2) for i in range(m)]) - const_term</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.cross_val"><code class="name flex">
<span>def <span class="ident">cross_val</span></span>(<span>X, method='kmeans', scoring='score', season='WINTER', folder='', verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to perform cross-validation of clustering methods
Args
X: pandas DataFrame containing data
method: clustering method to be validated
scoring
season:
verbose</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing
def cross_val(X, method=&#34;kmeans&#34;, scoring=&#34;score&#34;, season = &#34;WINTER&#34;, folder = &#39;&#39;, verbose=True):
    &#39;&#39;&#39;
    Method to perform cross-validation of clustering methods
    Args
        X: pandas DataFrame containing data
        method: clustering method to be validated
        scoring
        season:
        verbose
    &#39;&#39;&#39;

    if method == &#39;kmeans&#39;:
        estimator = KMeans(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;init&#34;: [&#34;k-means++&#34;, &#34;random&#34;],
                  &#34;n_init&#34;: [10, 50], &#34;max_iter&#34;: [100, 300, 1000], &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#34;kernel_kmeans&#34;:
        estimator = KernelKMeans(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;init&#34;: [&#34;k-means++&#34;, &#34;random&#34;],
                  &#34;n_init&#34;: [10, 50], &#34;max_iter&#34;: [100, 300, 1000], &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#39;bayesian_gmm&#39;:
        estimator = BayesianGaussianMixture(random_state=42)
        params = {&#34;n_components&#34;: [4, 5, 6, 7], &#34;covariance_type&#34;: [&#34;full&#34;],
                  &#34;n_init&#34;: [5, 10], &#34;max_iter&#34;: [100, 3000], &#34;init_params&#34;: [&#34;kmeans&#34;, &#34;random&#34;],
                  &#34;tol&#34;: [1e-3, 1e-7],
                  &#34;weight_concentration_prior_type&#34;: [&#34;dirichlet_process&#34;, &#34;dirichlet_distribution&#34;]}

    elif method == &#39;gmm&#39;:
        estimator = GaussianMixture(random_state=42, warm_start=True)
        params = {&#34;n_components&#34;: [4, 5, 6, 7], &#34;covariance_type&#34;: [&#34;full&#34;],
                  &#34;n_init&#34;: [5, 10], &#34;max_iter&#34;: [100, 300, 1000], &#34;init_params&#34;: [&#34;kmeans&#34;, &#34;random&#34;],
                  &#34;tol&#34;: [1e-3, 1e-5, 1e-7]}

    elif method == &#39;spectral&#39;:
        estimator = SpectralClustering(random_state=42)
        params = {&#34;n_clusters&#34;: [4, 5, 6, 7], &#34;affinity&#34;: [&#34;nearest_neighbors&#34;, &#34;rbf&#34;, &#34;precomputed&#34;],
                  &#34;n_neighbors&#34;: [10, 50], &#34;n_init&#34;: [10, 50]}

    else:
        estimator = None

    def make_generator(parameters):
        &#39;&#39;&#39;
        Method creating a generator on the fly returning all the combinations given the passed parameters
        Args:
            parameters: a dictionary containing the parameters to be passed
        &#39;&#39;&#39;
        if not parameters:
            yield dict()
        else:
            key_to_iterate = list(parameters.keys())[0]
            next_round_parameters = {p: parameters[p]
                                     for p in parameters if p != key_to_iterate}
            for val in parameters[key_to_iterate]:
                for pars in make_generator(next_round_parameters):
                    temp_res = pars
                    temp_res[key_to_iterate] = val
                    yield temp_res

    kf = KFold(n_splits=5, random_state=42, shuffle=True)
    _best_estimator, _best_score, _best_params = None, -np.inf, None

    def get_score(indexes, estimator, X, scoring):
        train_X, val_X = X[indexes[0]], X[indexes[1]]
        try:
            estimator.fit(train_X)
            labels = _est.predict(val_X)

            if scoring == &#34;score&#34;:
                scoring_fn = getattr(estimator, &#34;score&#34;, None)
                if callable(scoring_fn):
                    score = estimator.score(val_X)
            elif scoring == &#34;silhouette&#34;:
                score = silhouette_score(val_X, labels)

            elif scoring == &#34;ch&#34;:
                score = calinski_harabasz_score(val_X, labels)

            elif scoring == &#34;bic&#34;:
                scoring_fn = getattr(estimator, &#34;bic&#34;, None)
                if callable(scoring_fn):
                    score = - estimator.bic(val_X)
                else:
                    score = bic_score(val_X, labels,
                                      estimator.cluster_centers_ if hasattr(estimator,
                                                                            &#34;cluster_centers_&#34;) else estimator.means_)
        except:
            score = np.nan
        return score

    for i, param_grid in enumerate(make_generator(params)):
        _est = estimator.set_params(**param_grid)
        pool = Pool()
        scores = pool.map(partial(get_score, estimator=_est, X=X, scoring=scoring), kf.split(X))

        if verbose:
            print(&#34;\tCombination %d score: %.3f&#34; % (i + 1, np.mean(scores)))

        if np.mean(scores) &gt; _best_score:
            _best_estimator, _best_score, _best_params = deepcopy(_est), np.mean(scores), _est.get_params()

    try:
        os.makedirs(os.path.join(&#34;../models&#34;,season,folder))
    except OSError as oserr:
        pass
    finally:
        _best_estimator.fit(X)
        with open(&#34;../models/&#34; + season + &#34;/&#34; + folder + &#39;/&#39; + method + &#39;_model_&#39; + scoring + &#39;.pkl&#39;, &#39;wb&#39;) as f:
            pickle.dump(_best_estimator, f)

    print(&#34;Validation process ended with score {}\nBest parameters: {}&#34;.format(_best_score, _best_params))
    return _best_estimator</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.extract_regimes"><code class="name flex">
<span>def <span class="ident">extract_regimes</span></span>(<span>anomaly, method='kmeans', nb_regimes=5, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method clustering anomalies in different weather regimes</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>anomaly</code></strong></dt>
<dd>a pandas DataFrame containing an historical series of anomalies</dd>
<dt><strong><code>clustering_algo</code></strong></dt>
<dd>clustering algorithm to adopt. The class should expose the methods .fit(), .predict(), .fit_predict(), .fit_transform()</dd>
<dt><strong><code>nb_regimes</code></strong></dt>
<dd>number of different weather regimes to be identified, if 'estimator' in **kwargs, this parameter is ignored</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>a dictionary of further parameters, like the a pre_trained estimator</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>an array of regimes associated to each grid in the time series</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_regimes(anomaly, method=&#39;kmeans&#39;, nb_regimes=5, **kwargs):
    &#39;&#39;&#39;
    Method clustering anomalies in different weather regimes
    Args:
        anomaly: a pandas DataFrame containing an historical series of anomalies
        clustering_algo: clustering algorithm to adopt. The class should expose the methods .fit(), .predict(), .fit_predict(), .fit_transform()
        nb_regimes: number of different weather regimes to be identified, if &#39;estimator&#39; in **kwargs, this parameter is ignored
        **kwargs: a dictionary of further parameters, like the a pre_trained estimator

    Returns:
        an array of regimes associated to each grid in the time series
    &#39;&#39;&#39;
    if method == &#39;kmeans&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = KMeans(n_clusters=nb_regimes, random_state=42,
                                     tol=1e-5, n_init=50)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.inertia_, clustering_algo

    elif method == &#34;kernel_kmeans&#34;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = KernelKMeans(n_clusters=nb_regimes, random_state=42,
                                           tol=1e-5, n_init=50)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.inertia_, clustering_algo

    elif method == &#39;bayesian_gmm&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = BayesianGaussianMixture(n_components=nb_regimes, random_state=42, n_init=10,
                                                      covariance_type=&#39;full&#39; if &#39;covariance_type&#39; not in kwargs else
                                                      kwargs[&#39;covariance_type&#39;],
                                                      reg_covar=1e-3, max_iter=1000)
            clustering_algo.fit(anomaly)

        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        probas = clustering_algo.predict_proba(anomaly)

        return probas, clustering_algo.lower_bound_, clustering_algo.means_, clustering_algo.covariances_, clustering_algo

    elif method == &#39;gmm&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = GaussianMixture(n_components=nb_regimes, random_state=42, n_init=10,
                                              covariance_type=&#39;full&#39; if &#39;covariance_type&#39; not in kwargs else kwargs[
                                                  &#39;covariance_type&#39;],
                                              reg_covar=1e-3, max_iter=1000)
            clustering_algo.fit(anomaly)
        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        probas = clustering_algo.predict_proba(anomaly)

        return probas, clustering_algo.lower_bound_, clustering_algo.means_, clustering_algo.covariances_, clustering_algo

    elif method == &#39;spectral&#39;:
        if &#39;estimator&#39; not in kwargs:
            clustering_algo = SpectralClustering(n_clusters=nb_regimes, random_state=42, n_init=50,
                                                 affinity=&#39;rbf&#39; if &#39;affinity&#39; not in kwargs else kwargs[&#39;affinity&#39;])
            clustering_algo.fit(anomaly)

        else:
            clustering_algo = kwargs[&#39;estimator&#39;]
        labels = clustering_algo.predict(anomaly)
        return labels, clustering_algo.affinity_matrix_, clustering_algo

    else:
        pass</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.get_inertia"><code class="name flex">
<span>def <span class="ident">get_inertia</span></span>(<span>X, labels, centroids)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to compute the Sum of Squared Errors (SSE)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong></dt>
<dd>the dataset of clustered points</dd>
<dt><strong><code>labels</code></strong></dt>
<dd>the clustering assignments</dd>
<dt><strong><code>centroids</code></strong></dt>
<dd>the centroids associated to X</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_inertia(X, labels, centroids):
    &#39;&#39;&#39;
    Method to compute the Sum of Squared Errors (SSE)
    Args:
        X: the dataset of clustered points
        labels: the clustering assignments
        centroids: the centroids associated to X
    &#39;&#39;&#39;
    m = centroids.shape[0]
    return np.sum(
        [np.sum(distance.cdist(X[np.where(labels == i)], [centroids[i]], &#39;euclidean&#39;) ** 2) for i in range(m)])</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.get_statistics"><code class="name flex">
<span>def <span class="ident">get_statistics</span></span>(<span>folder, train, test)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to collect the statistics about the models' performances</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>folder</code></strong></dt>
<dd>name of the folder where there are pickle files of the model</dd>
<dt><strong><code>train</code></strong></dt>
<dd>the train dataset used to fit the model</dd>
<dt><strong><code>test</code></strong></dt>
<dd>the test dataset used to evaluate the performance on</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A pandas DataFrame containing the statistics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing
def get_statistics(folder, train, test):
    &#39;&#39;&#39;
    Method to collect the statistics about the models&#39; performances
    Args:
        folder: name of the folder where there are pickle files of the model
        train: the train dataset used to fit the model
        test: the test dataset used to evaluate the performance on

    Returns:
        A pandas DataFrame containing the statistics

    &#39;&#39;&#39;

    return pd.DataFrame.from_dict({model: performance_matrix(folder + &#39;/&#39; + model,
                                                      train.values, test.values) \
                            for model in os.listdir(folder) if
                            model.endswith(&#39;pkl&#39;)},
                           orient=&#39;index&#39;)</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.load_estimator"><code class="name flex">
<span>def <span class="ident">load_estimator</span></span>(<span>path_to_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to load an estimator from the pickle file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path_to_file</code></strong></dt>
<dd>file path to the pickle file</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the loaded estimator</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_estimator(path_to_file):
    &#39;&#39;&#39;
    Method to load an estimator from the pickle file
    Args:
        path_to_file: file path to the pickle file

    Returns:
        the loaded estimator
    &#39;&#39;&#39;
    with open(path_to_file, &#39;rb&#39;) as pickle_file:
        estimator = pickle.load(pickle_file)

    return estimator</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.performance_matrix"><code class="name flex">
<span>def <span class="ident">performance_matrix</span></span>(<span>estimator, train_X, val_X)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to evaluate an estimator, on different evaluation metrics</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>estimator</code></strong></dt>
<dd>the clustering algorithm to be tested, or the path to the pickle file to load the estimator</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a pandas DataFrame comparing several estimators</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def performance_matrix(estimator, train_X, val_X):
    &#39;&#39;&#39;
    Method to evaluate an estimator, on different evaluation metrics
    Args:
        estimator: the clustering algorithm to be tested, or the path to the pickle file to load the estimator

    Returns:
        a pandas DataFrame comparing several estimators
    &#39;&#39;&#39;

    if isinstance(estimator, str):
        estimator = load_estimator(estimator)

    labels = estimator.fit(train_X).predict(val_X)
    return {
        # &#34;Internal score&#34;: estimator.score(val_X, labels),
        &#34;Number of Clusters&#34;: estimator.get_params()[&#39;n_clusters&#39;
        if &#39;n_clusters&#39; in estimator.get_params() else &#39;n_components&#39;],
        &#34;BIC&#34;: bic_score(val_X, labels,
                         estimator.cluster_centers_ if hasattr(estimator, &#34;cluster_centers_&#34;) else estimator.means_),
        &#34;Silhouette Score&#34;: silhouette_score(val_X, labels),
        &#34;Calinski Harabsz Index&#34;: calinski_harabasz_score(val_X, labels),
        &#34;Inertia&#34;: get_inertia(val_X, labels,
                               estimator.cluster_centers_ if hasattr(estimator,
                                                                     &#34;cluster_centers_&#34;) else estimator.means_),

    }</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="modeling.utils.models.KernelKMeans"><code class="flex name class">
<span>class <span class="ident">KernelKMeans</span></span>
<span>(</span><span>n_clusters=3, max_iter=50, tol=0.001, random_state=None, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Kernel K-means</p>
<h2 id="reference">Reference</h2>
<p>Kernel k-means, Spectral Clustering and Normalized Cuts.
Inderjit S. Dhillon, Yuqiang Guan, Brian Kulis.
KDD 2004.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KernelKMeans(BaseEstimator, ClusterMixin):
    &#34;&#34;&#34;
    Kernel K-means

    Reference
    ---------
    Kernel k-means, Spectral Clustering and Normalized Cuts.
    Inderjit S. Dhillon, Yuqiang Guan, Brian Kulis.
    KDD 2004.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=3, max_iter=50, tol=1e-3, random_state=None,
                 kernel=&#34;linear&#34;, gamma=None, degree=3, coef0=1,
                 kernel_params=None, verbose=0):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.kernel = kernel
        self.gamma = gamma
        self.degree = degree
        self.coef0 = coef0
        self.kernel_params = kernel_params
        self.verbose = verbose

    @property
    def _pairwise(self):
        return self.kernel == &#34;precomputed&#34;

    def _get_kernel(self, X, Y=None):
        if callable(self.kernel):
            params = self.kernel_params or {}
        else:
            params = {&#34;gamma&#34;: self.gamma,
                      &#34;degree&#34;: self.degree,
                      &#34;coef0&#34;: self.coef0}
        return pairwise_kernels(X, Y, metric=self.kernel,
                                filter_params=True, **params)

    def fit(self, X, y=None, sample_weight=None):
        n_samples = X.shape[0]

        K = self._get_kernel(X)

        sw = sample_weight if sample_weight else np.ones(n_samples)
        self.sample_weight_ = sw

        rs = check_random_state(self.random_state)
        self.labels_ = rs.randint(self.n_clusters, size=n_samples)

        dist = np.zeros((n_samples, self.n_clusters))
        self.within_distances_ = np.zeros(self.n_clusters)

        for it in range(self.max_iter):
            dist.fill(0)
            self._compute_dist(K, dist, self.within_distances_,
                               update_within=True)
            labels_old = self.labels_
            self.labels_ = dist.argmin(axis=1)

            # Compute the number of samples whose cluster did not change
            # since last iteration.
            n_same = np.sum((self.labels_ - labels_old) == 0)
            if 1 - float(n_same) / n_samples &lt; self.tol:
                if self.verbose:
                    print(&#34;Converged at iteration&#34;, it + 1)
                break

        self.X_fit_ = X

        return self

    def _compute_dist(self, K, dist, within_distances, update_within):
        &#34;&#34;&#34;Compute a n_samples x n_clusters distance matrix using the
        kernel trick.&#34;&#34;&#34;
        sw = self.sample_weight_

        for j in range(self.n_clusters):
            mask = self.labels_ == j

            if np.sum(mask) == 0:
                raise ValueError(&#34;Empty cluster found, try smaller n_cluster.&#34;)

            denom = sw[mask].sum()
            denomsq = denom * denom

            if update_within:
                KK = K[mask][:, mask]  # K[mask, mask] does not work.
                dist_j = np.sum(np.outer(sw[mask], sw[mask]) * KK / denomsq)
                within_distances[j] = dist_j
                dist[:, j] += dist_j
            else:
                dist[:, j] += within_distances[j]

            dist[:, j] -= 2 * np.sum(sw[mask] * K[:, mask], axis=1) / denom

    def predict(self, X):
        K = self._get_kernel(X, self.X_fit_)
        n_samples = X.shape[0]
        dist = np.zeros((n_samples, self.n_clusters))
        self._compute_dist(K, dist, self.within_distances_,
                           update_within=False)
        return dist.argmin(axis=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.ClusterMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="modeling.utils.models.KernelKMeans.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None, sample_weight=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None, sample_weight=None):
    n_samples = X.shape[0]

    K = self._get_kernel(X)

    sw = sample_weight if sample_weight else np.ones(n_samples)
    self.sample_weight_ = sw

    rs = check_random_state(self.random_state)
    self.labels_ = rs.randint(self.n_clusters, size=n_samples)

    dist = np.zeros((n_samples, self.n_clusters))
    self.within_distances_ = np.zeros(self.n_clusters)

    for it in range(self.max_iter):
        dist.fill(0)
        self._compute_dist(K, dist, self.within_distances_,
                           update_within=True)
        labels_old = self.labels_
        self.labels_ = dist.argmin(axis=1)

        # Compute the number of samples whose cluster did not change
        # since last iteration.
        n_same = np.sum((self.labels_ - labels_old) == 0)
        if 1 - float(n_same) / n_samples &lt; self.tol:
            if self.verbose:
                print(&#34;Converged at iteration&#34;, it + 1)
            break

    self.X_fit_ = X

    return self</code></pre>
</details>
</dd>
<dt id="modeling.utils.models.KernelKMeans.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    K = self._get_kernel(X, self.X_fit_)
    n_samples = X.shape[0]
    dist = np.zeros((n_samples, self.n_clusters))
    self._compute_dist(K, dist, self.within_distances_,
                       update_within=False)
    return dist.argmin(axis=1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="modeling.utils" href="index.html">modeling.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="modeling.utils.models.bic_score" href="#modeling.utils.models.bic_score">bic_score</a></code></li>
<li><code><a title="modeling.utils.models.cross_val" href="#modeling.utils.models.cross_val">cross_val</a></code></li>
<li><code><a title="modeling.utils.models.extract_regimes" href="#modeling.utils.models.extract_regimes">extract_regimes</a></code></li>
<li><code><a title="modeling.utils.models.get_inertia" href="#modeling.utils.models.get_inertia">get_inertia</a></code></li>
<li><code><a title="modeling.utils.models.get_statistics" href="#modeling.utils.models.get_statistics">get_statistics</a></code></li>
<li><code><a title="modeling.utils.models.load_estimator" href="#modeling.utils.models.load_estimator">load_estimator</a></code></li>
<li><code><a title="modeling.utils.models.performance_matrix" href="#modeling.utils.models.performance_matrix">performance_matrix</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="modeling.utils.models.KernelKMeans" href="#modeling.utils.models.KernelKMeans">KernelKMeans</a></code></h4>
<ul class="">
<li><code><a title="modeling.utils.models.KernelKMeans.fit" href="#modeling.utils.models.KernelKMeans.fit">fit</a></code></li>
<li><code><a title="modeling.utils.models.KernelKMeans.predict" href="#modeling.utils.models.KernelKMeans.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>