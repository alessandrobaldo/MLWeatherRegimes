{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a7fe25",
   "metadata": {},
   "source": [
    "# Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93093f2",
   "metadata": {},
   "source": [
    "First thing to do is to initialize the configuration file under <strong>src/modeling/utils/config.json</strong>. Below are reported the two examples of config file for Winter and Summer seasons"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e3bedb5",
   "metadata": {},
   "source": [
    "{\n",
    "  \"readpath\": \"P:/CH/Weather Data/ERA-5/GEOPOTENTIAL\",\n",
    "  \"variable\": \"GeoPotential-500hPa\",\n",
    "  \"frequency\": \"hourly\",\n",
    "  \"months\": \"DecJanFeb\",\n",
    "  \"years\": \"1979-2021\",\n",
    "  \"lat\": [20 ,80],\n",
    "  \"long\": [-90, 30],\n",
    "  \"reduction\": \"PCA\", #or VAE\n",
    "  \"model\": \"gmm\", #[kmeans, bayesiangmm]\n",
    "  \"season\": \"SUMMER\",\n",
    "  \"training\": \"True\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "  \"readpath\": \"P:/CH/Weather Data/ERA-5/SLP\",\n",
    "  \"variable\": \"SLP\",\n",
    "  \"frequency\": \"hourly\",\n",
    "  \"months\": \"JunJulAug\",\n",
    "  \"years\": \"1979-2021\",\n",
    "  \"lat\": [20 ,80],\n",
    "  \"long\": [-90, 30],\n",
    "  \"reduction\": \"PCA\",\n",
    "  \"model\": \"gmm\",\n",
    "  \"season\": \"SUMMER\",\n",
    "  \"training\": \"True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da7be8",
   "metadata": {},
   "source": [
    "Now we proceed importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/') #sometimes needed when the ../src/ path is not found\n",
    "from modeling.utils.data import *\n",
    "from modeling.utils.plotting import *\n",
    "from modeling.utils.config import *\n",
    "#only useful for jupyter notebooks such that any change in the code behind does not need the kernel to be restarted\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3dabe",
   "metadata": {},
   "source": [
    "First step is to create the dataset by means of the build_data() function. The process is automatized. If the daily anomaly dataset was already created, the file is directly read, otherwise the process starts from the hourly observation file, it resamples it to daily, it evaluates the normal and finally the anomaly (pushing all the intermediate steps to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = build_data(normal_mode = 'flat', normal_freq = 'm')\n",
    "# normal_mode can either be flat or dynamic. If 'flat' normal_freq is ignored\n",
    "# if dynamic, with normal_freq we can govern the frequency: {'m': 'monthly', 'w': 'weekly', 'd': 'daily'} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49f651",
   "metadata": {},
   "source": [
    "We have then to weight the anomaly, to compensate for the elongation along the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = weighted_anomaly(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56490b0c",
   "metadata": {},
   "source": [
    "We have then to flat the spatial dimensions, in order to perform the dimensionality reduction through PCA, and for plotting the results of the modelign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_anomaly = flat_table(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89744b",
   "metadata": {},
   "source": [
    "Now the <strong>dimensionality reduction</strong> step. Either thorugh <strong>PCA</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dda569",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_anomaly = reduce_dim(pivot_anomaly, method = 'PCA', exp_variance = .9, season = 'SUMMER', load_est = 'pca_summer.pkl')\n",
    "#exp_variance could be either a float between 0 and 1, indicating the percentage of explained variance\n",
    "#or it could be hard-coded as an integer to specify the exact number of components\n",
    "#load_est is used to load a pre-fitted PCA estimator (to be used when we want to do inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661d49f",
   "metadata": {},
   "source": [
    "or through <strong>VAE</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4655fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_anomaly = reduce_dim(dt, method = 'VAE', season = 'SUMMER', model = \"sigma_vae_statedict_5_SLP\")\n",
    "#note here we pass directly the 3D dataset dt, because it works on the images\n",
    "#we have to specify the season (representing the folder where the VAE models are saved)\n",
    "#and the file of the VAE model we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c07ae9",
   "metadata": {},
   "source": [
    "We now proceed in diving in train and test datasets, in case we want to train from scratch the clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b12cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, pivot_train, pivot_test =\\ \n",
    "    train_test_split(reduced_anomaly, pivot_anomaly, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8827f4",
   "metadata": {},
   "source": [
    "The training loop adopts a 5-Fold Cross-Validation process, optimizing each model under 4 different scores. However, both the choice of the models to be optimized, and the scores are customizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['kmeans','bayesian_gmm','gmm']:\n",
    "    for scoring in  [\"score\", \"ch\", \"bic\", \"silhouette\"]:\n",
    "        estimator = cross_val(reduced_anomaly.values, method = model, scoring = scoring,\n",
    "                              season = season, folder = 'new folder name')\n",
    "#we specify again the season (root folder) and the folder where the models will be saved.\n",
    "#If it does not exist it is created automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b25d0",
   "metadata": {},
   "source": [
    "If we decide to skip the training procedure, or we simply just want to take a look at the performances of the models, we use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics(f'../models/{season}/wanted folder', train_X, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b68b4b",
   "metadata": {},
   "source": [
    "Finally, to visualize the centroids of one particular model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcc9c5",
   "metadata": {},
   "source": [
    "For <strong>K-Means</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a507e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = load_estimator(f'../models/{season}/SLP_pca_4pcs/kmeans_model_ch.pkl')\n",
    "outputs = extract_regimes(reduced_anomaly, method='kmeans', nb_regimes = None, estimator = estimator)\n",
    "labels, inertias, _ = outputs\n",
    "plot_regimes(pivot_anomaly, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d074c88",
   "metadata": {},
   "source": [
    "For <strong>Mixture Models</strong> (just change the name of the wanted model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = load_estimator(f'../models/{season}/SLP_pca_4pcs/gmm_model_silhouette.pkl')\n",
    "probas, elbo, means, covariances, _ = extract_regimes(reduced_anomaly, method='gmm',\n",
    "                                                      nb_regimes = None, estimator = estimator)\n",
    "labels = np.argmax(probas, axis=1)\n",
    "plot_regimes(pivot_anomaly, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223e004",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276ca3",
   "metadata": {},
   "source": [
    "To retrieve the EOFs and PCs we use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_, pcs = eofs(pivot_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb06b8b",
   "metadata": {},
   "source": [
    "To plot the EOFs we use instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952006fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_EOFS(eofs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7efeb9",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede83cd",
   "metadata": {},
   "source": [
    "First thing to do is to initialize the configuration file under <strong>src/dashboard/utils/config.json</strong>. Below are reported the two examples of config file for Winter and Summer seasons"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e881f457",
   "metadata": {},
   "source": [
    "{\n",
    "  \"season\": \"Winter\",\n",
    "  \"predictions\": \"predictions_winter\", #best models, alternatively predictions_VAE or predictions_PCA\n",
    "  \"months\": [1,2,12],\n",
    "  \"months_str\": \"DJF\",\n",
    "  \"regimes\": [\"AR\", \"NAO+\", \"NAO-\", \"SB\"]\n",
    "}\n",
    "\n",
    "{\n",
    "  \"season\": \"Summer\",\n",
    "  \"predictions\": \"predictions_summer\",\n",
    "  \"months\": [6,7,8],\n",
    "  \"months_str\": \"JJA\",\n",
    "  \"regimes\": [\"AL\", \"NAO-\", \"SB\", \"Zonal]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6af409",
   "metadata": {},
   "source": [
    "In order to start the dashboard, just run the following command in a terminal/command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run energy_dash.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa56bc5-89b6-4f5a-a893-b7ea5b2c1c40",
   "metadata": {},
   "source": [
    "The possible options to be selected in the dash board are: <strong>EU-7, BE, ES, FR, GE, IT, NE, UK</strong> to look for the countries quantifications, <strong>Sub-seasonal Forecasts</strong> to test the sub-seasonal forecast of ECMWF, <strong>Meteo-France</strong> to perform the comparison with Meteo-France predictions, <strong>Model Dynamics</strong> to examine the transition probabilities of each model, and <strong>Comparison True Measurements and Synthetic Data</strong> to perform a data exploration of the additional weather and energy data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
